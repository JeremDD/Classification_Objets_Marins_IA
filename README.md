<div align='center'>
    <h1><b> Classification vid√©o d'objets marins par intelligence artificielle  </b></h1>
    <img src='assets/images/picture_cover.webp' width="800"/>

![Python](https://badgen.net/badge/Python/3.12.3/blue?)
![YOLO](https://badgen.net/badge/YOLO/v9/yellow?)
![OpenCV](https://badgen.net/badge/OpenCV/1.26.4/red?)
![Roboflow](https://badgen.net/badge/Roboflow/Universe/purple?)
![PyCharm](https://badgen.net/badge/PyCharm/Professional/green?)
![Unity](https://badgen.net/badge/Unity/2022.3/black?)
![VLC](https://badgen.net/badge/VLC/3.0.21/orange?)
![SteamVR](https://badgen.net/badge/SteamVR/2.7.4/cyan?)

</div>

# üìñ Introduction

# üîß Impl√©mentation du modele de classification
## üëÅÔ∏è Installation Yolov9 :

Clone du repo Yolov9 :
```shell
git clone https://github.com/WongKinYiu/yolov9.git
cd yolov9
pip install -r requirements.txt -q
```
Comprend le mod√®le, les premiers poids, tous les outils d'analyse des r√©sultats d'entrainements ect ...

Les fichiers .pt sont des mod√®les "pr√©-entra√Æn√©s" : des fichiers qui contiennent un mod√®le qui a d√©j√† appris √† faire de la reconnaissance d'objets.  
Plusieurs versions sont dispo, adapt√©es √† diff√©rentes situations :

* Rapide mais moins pr√©cis ‚Üí Tiny (...-t).
* Equilibr√© ‚Üí Small ou Medium (...-s / ...-m).
* Pr√©cis mais √©nergivore ‚Üí Custom ou Enhanced (...-c / ...-e).

Pour la demo on prendra le fichier gelan-c-det.pt (techno Gelan, Custom et adapt√© √† la d√©tection d'objets).  
On les place dans un nouveau fichier "weights" dans le dossier Yolov9 :

```shell
mkdir -p yolov9/weights
wget -P yolov9/weights -q https://github.com/WongKinYiu/yolov9/releases/download/v0.1/gelan-c-det.pt
```

## üìö Installation Datasets :

J'ai ici utilis√© Roboflow Universe, une plateforme qui regroupe une communaut√© de plus de 200 000 ensembles de donn√©es de vision par ordinateur.  

Apres avoir choisi un dataset sur Roboflow, plusieurs options s'offrent √† nous :
- Format zip
- Terminal
- Code Jupiter_Notebook (que j'ai choisi et dont le script est dans : [Datasets_download.ipynb](yolov9%2FDatasets_download.ipynb))

Une identification sera demander lors de l'execution du code, il suffit dans le cas o√π vous avez un compte de copier 
le lien sur lequel vous √™tes envoy√© dans le terminal et sinon de cr√©er un compte sur roboflow.

Lors du choix d'un dataset v√©rifier que :

 * Les donn√©es sont repr√©sentatives de la r√©alit√©. Si le dataset est d√©s√©quilibr√© (par exemple, sur-repr√©sentation d'une classe), le mod√®le risque de ne pas g√©n√©raliser correctement.
(cas que j'ai rencontr√© avec les datasets SMD et MODD qui sur-repr√©sentaient les cargos pas vrm pr√©sent dans le bassin d'Arcachon ...)


 * Plus un mod√®le est complexe, plus il a besoin de donn√©es pour apprendre efficacement. Cependant, avoir trop de donn√©es peut entra√Æner des temps de traitement excessifs ou des difficult√©s d'analyse.

**Les datasets sont √† installer dans le repertoire yolov9 !**

## üèãÔ∏è‚Äç‚ôÇÔ∏è Training :

Pour la demo un petit dataset simple mais efficace sera utilis√©. 
Il est important de bien choisir/trouver son dataset, cela a √©t√© la principale difficult√© de ce projet !

Voil√† la commande de base d'un simple entrainement les differents parametres sont dans le code train.py et sont √† adapter suivant les besoins.

```shell
cd yolov9
python train.py \
--batch-size 8 \
--epochs 50 \
--img 640 \
--device 0 \
--data "Sea-Vessels-Dataset-2/data.yaml" \
--weights "weights/gelan-c-det.pt" \
--cfg "models/detect/gelan-c.yaml" \
--hyp "data/hyps/hyp.scratch-high.yaml" 
```

## üîé Detection :

Le script suivant est le lancement du code detect.py classique dans la console :
```shell
cd yolov9
python detect.py \
--img 1280 \
--conf 0.5 \
--device 0 \
--weights "runs/train/exp/weights/best.pt" \
--source "videos/Exemple_port.mp4" \
--view-img \
--nosave 
```

# üì∑ Application sur un flux vid√©o en direct : cameras IP (via RTSP)

Une fois le mod√®le √©tablit l'objectif √©tait d'appliquer celui-ci √† des cam√©ras IP.
Pour r√©cup√©rer le flux vid√©o des cam√©ras on utilisera le protocole RTSP (Real-Time Streaming Protocol).

Pour ce faire le script detect.py (utilis√© pour "appliquer" le modele de classification a une image/video) √† √©t√© modifi√© affin de pouvoir s'appliquer √† un flux vid√©o live comme le RTSP.  
L'option ```camera_urls``` a donc √©t√© introduite pour permettre le traitement des flux vid√©o provenant d'URL (comme des cam√©ras IP).

```python
def run(weights=ROOT / 'yolo.pt', source=ROOT / 'data/images', camera_urls=None, ...):
    if camera_urls:
        source = camera_urls[0]
    else:
        source = str(source)
    ...
```
Voir [Rapport_ProjetMagellan_RTSP_CODEC.pdf](Ressources_Rapports/Rapport_ProjetMagellan_RTSP_CODEC.pdf), pour quelques infos suppl√©mentaires.

# ‚ñ∂Ô∏è Affichage

Pour l'affichage 2 m√©thodes ont √©t√© pens√©es :
* **Interface Web :** Une plut√¥t pour la comparaison et l'√©tude de r√©sultats des entrainements. 
* **Environnement Virtuel :** L'autre plut√¥t pour l'immersion dans le point de vue du drone (affin de mieux observer les alentours voir d'envisager de le piloter).

## ‚ú® Mini Interface Web : Streamlit

L'objectif est d'avoir un apercu clair de ce qui est ou a √©t√© r√©alis√©.
L'interface comprend diff√©rents curseurs/boutons afin de r√©gler :
* Le **mod√®le** √† utiliser.
* Le **m√©dia** sur lequel lancer la d√©tection.
* La valeur du **seuil de confiance** (filtre les d√©tections en fonction du niveau de confiance attribu√© √† chaque pr√©diction).
* S'il faut ou non **enregistrer** les r√©sultats
* L'affichage des **performances**

## ü•Ω R√©alit√© virtuelle : Unity










# ‚ùó Bugs rencontr√©s

**Terminal utilis√© : WSL**  
Install : Windows Powershell admin =>  ```wsl --install```.  
Pour l'utiliser sur Pycharm : ```Ctrl + Alt + S``` => ```Tools``` => ```Terminal``` => ```Shell path : wsl.exe```

--------------------------------------------------------------

**M√©thode getsize() deprecated dans Pillow**  
```AttributeError: 'FreeTypeFont' object has no attribute 'getsize'```  
La m√©thode getsize() utilis√©e dans le module utils/plots.py, est d√©pr√©ci√©e dans les versions r√©centes de Pillow.  
Elle √©tait utilis√©e pour obtenir la taille d'un texte (largeur et hauteur) dans le cadre de l'annotation des images.  
Elle a √©t√© remplac√©e par getbbox() qui renvoie la bo√Æte englobante du texte.
```python
# Ancien code :
w, h = self.font.getsize(label)  # text width, height

# Nouveau code :
bbox = self.font.getbbox(label)  # text bounding box
w, h = bbox[2] - bbox[0], bbox[3] - bbox[1]  # width = x2 - x1, height = y2 - y1
```
La m√©thode getbbox() renvoie les coordonn√©es de la bo√Æte englobante sous la forme (x1, y1, x2, y2).  
Pour obtenir la largeur et la hauteur du texte, nous calculons la diff√©rence entre les coordonn√©es x2 et x1 (largeur), et y2 et y1 (hauteur).

--------------------------------------------------------------

**torch.cuda.amp.GradScaler() and torch.cuda.amp.autocast() deprecated**  
```torch.cuda.amp.GradScaler(args...) is deprecated. Please use torch.amp.GradScaler('cuda', args...) instead.```
```torch.cuda.amp.autocast(args...) is deprecated. Please use torch.amp.autocast('cuda', args...) instead.```
PyTorch a d√©plac√© certaines fonctionnalit√©s dans un nouveau namespace torch.amp pour une meilleure gestion du calcul en pr√©cision mixte (AMP).  
Ce correctif assure la compatibilit√© avec les versions r√©centes de PyTorch.
```python
# Ancien code :
scaler = torch.cuda.amp.GradScaler(enabled=amp)
# Nouveau code :
scaler = torch.amp.GradScaler(enabled=amp)
...
# Ancien code :
with torch.cuda.amp.autocast(amp):

# Nouveau code :
with torch.amp.autocast('cuda', enabled=amp):
```

```shell
git add .
now = `date`
git commit -m "Update du $now"
git push origin main
```
